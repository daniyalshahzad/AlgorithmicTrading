{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8fe97ea0-a96f-48ef-9e90-43cd60fb0549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b47eae8-3aee-4200-bf00-51cb6d1e3628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f41521e59d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fc3062c-ed77-4808-b62e-75d6777580bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples = observations\n",
    "#num_of_news = news for a given point\n",
    "def create_fake_data(samples = 5, num_of_news = 5, embedding_dim = 16):\n",
    "    labels = torch.randint(0, 2, (samples,))\n",
    "    data = torch.randn(samples, num_of_news, embedding_dim)\n",
    "    return data, labels\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0afc10dd-6303-4196-b5f9-f837122a2752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.query = nn.Parameter(torch.randn(embed_dim)) #Query vector\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        embeddings: shape (batch_size, N, d)\n",
    "                    - batch_size: how many samples in one batch (b)\n",
    "                    - N: number of headlines per sample\n",
    "                    - d: embedding dimension\n",
    "        Returns: shape (batch_size, d)\n",
    "                 A single aggregated vector for each sample.\n",
    "        \"\"\"\n",
    "        batch_size, N, d = embeddings.shape\n",
    "        \n",
    "        ''' In attention mechanisms, we want to measure how relevant or similar each input (e.g., a headline embedding) is to a query vector ð‘ž\n",
    "        q. The dot product is a natural way to compute this similarity. '''\n",
    "        dot_products = torch.einsum('bnd,d->bn', embeddings, self.query) / d**0.5\n",
    "        \n",
    "        attention_weights = F.softmax(dot_products, dim=1) #gives me a vector of (b, n)\n",
    "        aggregated = torch.einsum('bn,bnd->bd', attention_weights, embeddings) #weight sum across all n\n",
    "        \n",
    "        return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9431f76-52af-4692-a2c5-c14627ff5181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HeadlineClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(HeadlineClassifier, self).__init__()\n",
    "        \n",
    "        self.attention = DotProductAttention(embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, embeddings): #embeddings = input data\n",
    "        \n",
    "        agg_vector = self.attention(embeddings) #(b, d)\n",
    "        \n",
    "        x = F.relu(self.fc1(agg_vector))\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        ''' The reason we donâ€™t apply an activation function (like softmax) in the\n",
    "        final layer is that PyTorchâ€™s loss functions (like nn.CrossEntropyLoss) expect raw logits as input. '''\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c6cbe7d-1a3e-430f-ba3a-06a2ff567ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "num_headlines = 5\n",
    "embed_dim = 16\n",
    "\n",
    "embeddings, labels = create_fake_data(num_samples, num_headlines, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a34d958a-6b06-4214-98c5-32e8b291a498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efe8b4ec-b874-487f-a8d6-1072b9524daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7a6fe78-92fb-4afc-8166-6f3d1dc02310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "num_classes = 2  # binary classification\n",
    "model = HeadlineClassifier(embed_dim, hidden_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0e2b036-bac8-4993-9326-550b11da05ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53d9cc48-69a7-4c93-a7dc-33cdc642e743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0484, Accuracy: 52.00%\n",
      "Epoch [2/5], Loss: 0.0493, Accuracy: 51.00%\n",
      "Epoch [3/5], Loss: 0.0483, Accuracy: 51.00%\n",
      "Epoch [4/5], Loss: 0.0485, Accuracy: 51.00%\n",
      "Epoch [5/5], Loss: 0.0480, Accuracy: 52.00%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_embeddings, batch_labels in dataloader:\n",
    "        \n",
    "        logits = model(batch_embeddings)\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "        total += batch_embeddings.size(0)\n",
    "        \n",
    "        \n",
    "    epoch_loss /= total\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Loss: {epoch_loss:.4f}, \"\n",
    "          f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164c1b1-1f75-48bb-8420-62ec3c450d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
